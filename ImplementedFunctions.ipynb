{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Lejla Dževlan\n",
        "Task for Entropy 387- position Machine Learning Engineer\n",
        "ML Challenge #2 - (Undirected Graph)"
      ],
      "metadata": {
        "id": "UT807EWq7GNk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Functions\n",
        "There are several functions below, that were requested as such in the assigned task, to be implemented manually, or served and were useful in the creation of some other functionalities. It will briefly be explained what the function does - before its actual implementation.\n",
        "\n",
        "Before running functions, some libraries should be imported and that is done once at the very beginning, so there is no need to import it in every function separately."
      ],
      "metadata": {
        "id": "28_5AhI4Znb1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "YXouDIFver8q"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import logging\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy as sp\n",
        "from scipy import sparse\n",
        "import numpy as np\n",
        "from scipy.sparse.linalg import eigs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function **read_json** will not be much explained because it is nothing new, it just reads json file from the given path of uploaded json file."
      ],
      "metadata": {
        "id": "SdvyyPrqaR2M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_json(file):\n",
        "      try:\n",
        "        with open(file, 'r') as f:\n",
        "            return json.load(f)\n",
        "      finally:\n",
        "        print('Done reading')"
      ],
      "metadata": {
        "id": "nb0GhXLje--S"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**THIS IS A PART OF THE CODE TO READ FROM JSON, AND MAKE PD DATAFRAMES FOR USING IN OTHER FUNCTIONS, DATA FRAMES ARE df_1 (nodes), and df_2 (links).**"
      ],
      "metadata": {
        "id": "TSg83QxSfZzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "d = read_json('/content/challenge_graph.json')\n",
        "df_1 = pd.DataFrame(d['nodes'])\n",
        "df_2 = pd.DataFrame(d['links'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4mCBPschfZ5",
        "outputId": "78b1f375-0649-4724-b484-eb24f6ecfaea"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done reading\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First in the row, is the function **VertexEdgeCounts** which accepts file that is actually path to the uploaded json file i.e. '/content/challenge_graph.json'. The function returns 2 numbers which are number of nodes and number of edges respectively.\n",
        "(This is for json file where there are 2 dictionaries, nodes and links, could be made for more generic case but I suppose this is okay for the task for now).\n",
        "\n",
        "At the end of implemented function, there is example of using this function for a graph from the task. \n",
        "Note: challenge_graph.json needs to be uploaded."
      ],
      "metadata": {
        "id": "XaxlJOfte4HL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def VertexEdgeCounts(file):\n",
        "    d = read_json(file)\n",
        "    df_1 = pd.DataFrame(d['nodes'])\n",
        "    df_2 = pd.DataFrame(d['links'])\n",
        "    len_nodes = df_1.shape[0]                       \n",
        "    len_edges = df_2.shape[0]\n",
        "    print(\"Number of nodes in your .json file is \", len_nodes)\n",
        "    print(\"Number of edges in your .json file is \", len_edges)\n",
        "    return(len_nodes, len_edges)\n",
        "\n",
        "#m = VertexEdgeCounts('/content/challenge_graph.json')\n",
        "#print(m)"
      ],
      "metadata": {
        "id": "NaNXcnJ3e4rb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function **VertexDegree** returns the degree of vertex specified in input, from the graph given with its path."
      ],
      "metadata": {
        "id": "paLz6YbJbFlT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def VertexDegree(file, node):\n",
        "    vertex = node;\n",
        "    brojac = 0;\n",
        "    brojac_2 = 0;\n",
        "    degree_of_node = 0;\n",
        "    return_dict = read_json(file)\n",
        "    d = return_dict \n",
        "    df_1 = pd.DataFrame(d['nodes'])\n",
        "    df_2 = pd.DataFrame(d['links'])\n",
        "\n",
        "    br_source = df_2['source'].str.count(vertex)\n",
        "    br_source_niz = np.array(br_source)\n",
        "    br_target = df_2['target'].str.count(vertex)\n",
        "    br_target_niz = np.array(br_target)\n",
        "\n",
        "   \n",
        "    for i in range(len(br_source_niz)):\n",
        "       if(br_source_niz[i] == 1):  \n",
        "            brojac = brojac + 1;\n",
        "\n",
        "    for i in range(len(br_target_niz)):\n",
        "        if(br_target_niz[i] == 1):  \n",
        "              brojac_2 = brojac_2 + 1;\n",
        "\n",
        "\n",
        "    degree_of_node = brojac + brojac_2;\n",
        "    print('Degree of node', node, 'is: ', degree_of_node)\n",
        "    return degree_of_node\n",
        "\n",
        "#VertexDegree('/content/challenge_graph.json', \"48633953191\")"
      ],
      "metadata": {
        "id": "nl46j6XWfD67"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function **CountShowIsolates** returns list of isolates (id names) - list of strings, nodes with degree zero. If there is no isolates in graph returned list is empty."
      ],
      "metadata": {
        "id": "pQYKBAEpbnLb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def CountShowIsolates(file):\n",
        "\n",
        "  d = read_json(file)\n",
        "  df_1 = pd.DataFrame(d['nodes'])\n",
        "  df_2 = pd.DataFrame(d['links'])\n",
        "  pomocni_id = \"\"\n",
        "  pomocni_degree = 0;\n",
        "  isolates = []    \n",
        "\n",
        "  for i in range(len(df_1['id'])):\n",
        "    pomocni_id = df_1['id'][i]\n",
        "    #print(pomocni_id)\n",
        "    #pomocni_degree = VertexDegree(file, pomocni_id)\n",
        "    #print(pomocni_degree)\n",
        "    #if (pomocni_degree == 0):\n",
        "    #  niz_isolates.append(pomocni_id)\n",
        "\n",
        "    if (VertexDegree(file, pomocni_id) == 0):\n",
        "      isolates.append(pomocni_id)\n",
        " \n",
        "  if isolates:\n",
        "       print('Number of isolates is not zero, and isolates are the nodes with following ids: ', isolates)\n",
        "  else:\n",
        "       print('This graph does not have isolates (no nodes with degree = 0)')\n",
        "\n",
        "  return isolates\n",
        "\n",
        "\n",
        "#izolati = CountShowIsolates('/content/challenge_graph.json');"
      ],
      "metadata": {
        "id": "gt6Ii_sCfFgq"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function **ThisIsIndex_improved** accepts df_1 which is dictionary of nodes with the column ['id'], and the second input is value - which is string and the name of node whose index we want to know. This function is improved with respect to ThisIsIndex because if there are several occurences of the node name, the function returns last index from the list. "
      ],
      "metadata": {
        "id": "4Ndw1r4mdH8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ThisIsIndex_improved(df_1, value):\n",
        "\n",
        "  value = str(value)\n",
        "  find_output = df_1['id'].str.find(value)\n",
        "  \n",
        "  lista_indexa = []\n",
        "  lista_vrijednosti = []\n",
        "  for i in range(len(find_output)):\n",
        "    if (find_output[i] != -1):\n",
        "      lista_indexa.append(i)\n",
        "      lista_vrijednosti.append(find_output[i])\n",
        "  zadnji_index = lista_indexa[len(lista_indexa)-1]\n",
        "  return zadnji_index"
      ],
      "metadata": {
        "id": "gqOVTByHf7q8"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function **Graph_from_JsonFile** returns the object of type graph (using networkx library), from the accepted path (path to the uploaded json file)."
      ],
      "metadata": {
        "id": "ci3l1cvtdqTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Graph_from_JsonFile(filename):                \n",
        "    with open(filename) as f:\n",
        "        js_graph = json.load(f)\n",
        "        G = nx.Graph()\n",
        "    return nx.node_link_graph(js_graph)"
      ],
      "metadata": {
        "id": "fi7bqVemg4c2"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function **CompareNumpyScipyMatrices** is used to compare matrix of type np.array() and the sparse csr_matrix, returns substraction of two matrices type sparse csr_matrix, and if the result is empty - matrices are the same, if not - result of the functions can show which values are different."
      ],
      "metadata": {
        "id": "SU0YiY_3e-Bg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def CompareNumpyScipyMatrices(matrix_numpy, matrix_sparse):\n",
        "  from scipy import sparse\n",
        "  m_transformed = sparse.csr_matrix(matrix_numpy)\n",
        "  difference = m_transformed - matrix_sparse\n",
        "  return difference"
      ],
      "metadata": {
        "id": "l3aAdQd5g_aD"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function **ReturnDataFramesFromPath** returns two pandas dataframes - nodes and links, seperated, from the accepted json file given with its path."
      ],
      "metadata": {
        "id": "1IlM0p2JfnGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ReturnDataFramesFromPath(path):\n",
        "  import pandas as pd\n",
        "  d = read_json(path)\n",
        "  df_1 = pd.DataFrame(d['nodes'])\n",
        "  df_2 = pd.DataFrame(d['links'])\n",
        "  return df_1, df_2"
      ],
      "metadata": {
        "id": "RhquWeBMhmcD"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function **ReturnIndexFromNode** has two input arguments, first one is pandas dataframe of nodes (the first output of the above function), and the second argument is node in given with its name in string type."
      ],
      "metadata": {
        "id": "A7Nlo6vFfyIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ReturnIndexFromNode(df_1_in, node_in):\n",
        "  df_1 = df_1_in\n",
        "  for i in range(len(df_1['id'])):\n",
        "    if(df_1['id'][i] == node_in):\n",
        "      return i"
      ],
      "metadata": {
        "id": "3KS-E4Keh3aj"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function **ReturnNodeFromIndex** is the opposite of the above one, this function from given df_1 pandas dataframe (nodes), and index of the wanted node, returns string which is name of the node."
      ],
      "metadata": {
        "id": "aG8IF7M9gD18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ReturnNodeFromIndex(df_1_in, index_in):\n",
        "  return df_1_in['id'][index_in]"
      ],
      "metadata": {
        "id": "k9ze7NXqiidT"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function ReturnEdges is also used in many other functions, and from given df_1 and df_2 (pandas dataframes of nodes and links, returns 2 lists. \n",
        "First returned list is actually list of pairs (source, target), while the second returned list is list of pairs of indexes of connected nodes."
      ],
      "metadata": {
        "id": "yzMsXsCXgSNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ReturnEdges(df_1_in, df_2_in):\n",
        "  df_1 = df_1_in\n",
        "  df_2 = df_2_in\n",
        "  izvori = df_2['source']\n",
        "  targeti = df_2['target']\n",
        "  edges = [0 for _ in range(len(izvori))]                                       \n",
        "  edges_indexi = [0 for _ in range(len(izvori))]  \n",
        "\n",
        "  for i in range(len(izvori)):\n",
        "    edges[i] = [izvori[i], targeti[i]]\n",
        "    prvi_index = ReturnIndexFromNode(df_1, izvori[i])\n",
        "    drugi_index = ReturnIndexFromNode(df_1, targeti[i])\n",
        "    edges_indexi[i] = [prvi_index, drugi_index]\n",
        "  return edges, edges_indexi                                                    "
      ],
      "metadata": {
        "id": "TUak8j7TiyyJ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Funtion **ReturnAdjacencyList** returns list of lists, for each node, the list for that node is actually list of nodes connected with it, the input arguments are dataframes df_1, and df_2 which represent nodes and links respectively."
      ],
      "metadata": {
        "id": "4f3tJVu5gyy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ReturnAdjacencyList(df_1_in, df_2_in):\n",
        "  df_1 = df_1_in\n",
        "  df_2 = df_2_in \n",
        "  node_data = [[] for _ in range(len(df_1['id']))]\n",
        "  pomocna = ReturnEdges(df_1, df_2)\n",
        "  edges_indexi = pomocna[1]\n",
        "  for n1, n2 in edges_indexi:\n",
        "     node_data[n1].append(n2)\n",
        "     node_data[n2].append(n1)\n",
        "  #for x in enumerate(node_data):\n",
        "  # print(x)\n",
        "  return node_data"
      ],
      "metadata": {
        "id": "MGtCQ5p3i8yW"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function **AdjMatrixFromAdjList** actually use **ReturnAdjacencyList** in order to give adjacency list in matrix form, which is sometimes useful. The value in row i and column j is 1 if nodes i and j are connected with links (edges). "
      ],
      "metadata": {
        "id": "CA2L8vSRhORJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def AdjMatrixFromAdjList(df_1_in, df_2_in):\n",
        "  lista = ReturnAdjacencyList(df_1_in, df_2_in)\n",
        "  br_elemenata = len(lista)\n",
        "  Matrica = np.zeros((br_elemenata, br_elemenata))\n",
        "\n",
        "  for node in range(len(lista)):\n",
        "    for i in range(len(lista[node])):\n",
        "      pomocna = lista[node][i]\n",
        "      Matrica[node][pomocna] = 1\n",
        "  return Matrica\n"
      ],
      "metadata": {
        "id": "4RHgBdnkjHCF"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function **bfsGivenRoot** performs breadth-first-search algorithm starting from given root (node). First two input parameters are pandas data frame df_1 and df_2 which are nodes and links respectively (can be obtained using one of the functions above)."
      ],
      "metadata": {
        "id": "orGLeBCJhiAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bfsGivenRoot(df_1, df_2, root):                                               #node data je izlaz od ReturnAdjacencyList(df_1, df_2)\n",
        "  node_data = ReturnAdjacencyList(df_1, df_2)\n",
        "  connections = node_data\n",
        "  length = len(node_data)\n",
        "  nodes = [i for i in range(length)]\n",
        "  queue=[]\n",
        "\n",
        "  discovered = [False]*len(nodes)\n",
        "  discovered[root] = True\n",
        "  queue.append(root)\n",
        "  index = 0                                                                       \n",
        "\n",
        "  while index < len(queue):\n",
        "    #dequeue\n",
        "    current = queue[index]\n",
        "    index += 1\n",
        "\n",
        "    #check all edges of current\n",
        "    for node in connections[current]:\n",
        "      if not discovered[node]:\n",
        "        discovered[node] = True\n",
        "        queue.append(node)\n",
        "  return queue"
      ],
      "metadata": {
        "id": "R2vK9s4NjMx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function bfsRecursive is function which recursively implements breadth-first-search, from root node, but also checking all of its neighbors."
      ],
      "metadata": {
        "id": "4OlPC9JZkEln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bfsRecursive(connections, temp, root, visited):\n",
        "  visited[root] = True\n",
        "  temp.append(root)\n",
        "\n",
        "  for i in connections[root]:                                                    #for each neighbor start\n",
        "    if (visited[i] == False):\n",
        "      temp = bfsRecursive(connections, temp,i,visited)\n",
        "  return temp"
      ],
      "metadata": {
        "id": "inrQyGQQjSYP"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function **FindConnectedComponents** uses bfsRecursive function to find connected components in graph and returns the list of connected components. If length of list is one, then graph is *connected*, so it can also be used in terms to test if the graph is connected (which was one of the tasks)."
      ],
      "metadata": {
        "id": "mmEs_xzvkEB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def FindConnectedComponents(df_1, df_2):\n",
        "  connections = ReturnAdjacencyList(df_1, df_2)\n",
        "  connections_ulaz = connections\n",
        "  \n",
        "  visited = [False]*len(connections)                 #[]                                  #discovered = [False]*len(nodes)\n",
        "  connected_components = []\n",
        "  for i in range(len(connections)):\n",
        "    visited.append(False)\n",
        "  \n",
        "  for v in range (len(connections)):\n",
        "    if (visited[v] ==False):\n",
        "      temp = []\n",
        "      connected_components.append(bfsRecursive(connections_ulaz, temp, v, visited))\n",
        "  return connected_components\n",
        "\n",
        "#print(list(nx.connected_components(G)))\n",
        "#print(len(list(nx.connected_components(G))))"
      ],
      "metadata": {
        "id": "1Dmkq6yujffs"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function IsConnected uses df_1, df_2 and root node to check if the graph is connected, the returned value is True if graph is connected."
      ],
      "metadata": {
        "id": "nKKhisiE4R8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def IsConnected(df_1, df_2):                \n",
        "    IsConnected = False\n",
        "    connections = ReturnAdjacencyList(df_1, df_2)\n",
        "    connected_components = FindConnectedComponents(df_1, df_2)\n",
        "    length_con_comp = len(connected_components)\n",
        "    if (len(connected_components) > 0):\n",
        "      print('This graph is not connected, and it has', length_con_comp, 'connected components.')\n",
        "    if (len(connected_components)  == 0):\n",
        "      print('This graph is connected')\n",
        "      IsConnected = True\n",
        "    return IsConnected\n",
        "    \n",
        "#IsConnected(df_1, df_2)"
      ],
      "metadata": {
        "id": "nuq8SH_24RQt"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function **betweenness_centrality** from pandas dataframes df_1 and df_2 (nodes and links respectively), returns list of betweenness centralities calculated for each node."
      ],
      "metadata": {
        "id": "Z2PqC7ATnS5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def betweenness_centrality(df_1_in, df_2_in):\n",
        "    #this is the only function where I have not been able to avoid using the networkx library, for my attempts to code it - check \"radna\" versions\n",
        "\n",
        "    Mat = AdjMatrixFromAdjList(df_1, df_2)\n",
        "    Adj_matrix = nx.from_numpy_array(Mat)                                           \n",
        "\n",
        "    betweenness = dict.fromkeys(Adj_matrix, 0.0)  \n",
        "\n",
        "    nodes = Adj_matrix.nodes\n",
        "   \n",
        "    for s in nodes:\n",
        "        Stack = []\n",
        "        \n",
        "        predessor = {}\n",
        "        \n",
        "        for v in Adj_matrix:\n",
        "            predessor[v] = []\n",
        "        \n",
        "        sigma_val = dict.fromkeys(Adj_matrix, 0.0) \n",
        "        \n",
        "        Dist = {}\n",
        "        sigma_val[s] = 1.0\n",
        "        Dist[s] = 0\n",
        "        Queue = [s]\n",
        "        \n",
        "        #Use the BFS algorithm while Queue is not empty, do:\n",
        "        while Queue:  \n",
        "            #dequeue top element(node) from Q and push v onto stack S\n",
        "            v = Queue.pop(0)\n",
        "            Stack.append(v)\n",
        "            \n",
        "            d = Dist[v] \n",
        "            sigmav = sigma_val[v]\n",
        "\n",
        "            #For each node g such that is an edge in G from g to v, do\n",
        "            for g in Adj_matrix[v]:\n",
        "                if g not in Dist:\n",
        "                    #enqueue\n",
        "                    Queue.append(g)\n",
        "                    #set distance of g to (d+1)\n",
        "                    Dist[g] = d + 1\n",
        "                    # this gives shortest path\n",
        "                if Dist[g] == d + 1: \n",
        "                    #setting σ(s, w) to σ(s, w) + σ(s, v)\n",
        "                    sigma_val[g] += sigmav\n",
        "                    #add predessors(append g to Pred[g])\n",
        "                    predessor[g].append(v) \n",
        "        #calculation of centrality  \n",
        "        delta = dict.fromkeys(Stack, 0)\n",
        "        #while stack is not empty, pop g off stack\n",
        "        while Stack:\n",
        "            g = Stack.pop()\n",
        "            magic = (1.0 + delta[g]) / sigma_val[g]\n",
        "            #for all nodes v in Pred(g) set δ(v) to δ(v) + coefficient\n",
        "            for v in predessor[g]:\n",
        "                delta[v] += sigma_val[v] * magic\n",
        "            if g != s:\n",
        "                betweenness[g] += delta[g]\n",
        "    #Scaling \n",
        "    scale = 0.5\n",
        "    for b in betweenness:\n",
        "         betweenness[b] *= scale\n",
        "    b_centrality=list(betweenness.values())\n",
        "\n",
        "    return b_centrality      \n",
        "\n",
        "#Testing\n",
        "#G = Graph_from_JsonFile('/content/challenge_graph.json')\n",
        "#betw = nx.betweenness_centrality(G)\n",
        "#max_value = max(betw, key=betw.get)\n",
        "#print(max_value)\n",
        "\n",
        "#print(ThisIsIndex_improved(df_1, max_value))\n",
        "\n",
        "#from_man = betweenness_centrality(df_1, df_2)\n",
        "#max_index = 0\n",
        "#nadjeni_max = 0\n",
        "#for i in range(len(moja)):\n",
        "#  if(moja[i] > nadjeni_max):\n",
        "#    max_index = i\n",
        "#    nadjeni_max = moja[i]\n",
        "    \n",
        "#print(nadjeni_max)\n",
        "#print(max_index)"
      ],
      "metadata": {
        "id": "g5FsSnZDj42O"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function **eigenvector_centrality** from pandas dataframes df_1 and df_2 (nodes and links respectively), using eigs from scipy.sparse.linalg returns list of eigenvector centralities calculated for each node."
      ],
      "metadata": {
        "id": "jBmo7VyWnnqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eigenvector_centrality(df_1, df_2):\n",
        "    #note: import libraries\n",
        "    M_matrix = AdjMatrixFromAdjList(df_1, df_2)\n",
        "    M = sparse.csr_matrix(M_matrix)\n",
        "\n",
        "    #calculate eigen values\n",
        "    e_value, e_vector = sp.sparse.linalg.eigs(M.T, k = 1, which='LR',maxiter=50, tol=0)     \n",
        "    duzina_1 = len(e_value)\n",
        "    duzina_2 = len(e_vector)\n",
        "\n",
        "    final_val = [0 for _ in range(duzina_2)]\n",
        "    largest_value = e_vector.flatten().real\n",
        "\n",
        "    norm = np.sign(largest_value.sum()) * np.linalg.norm(largest_value)           #scaling can be performed for more stable results\n",
        "    \n",
        "    for i in range(duzina_2):\n",
        "      final_val[i] = largest_value[i]/norm\n",
        "    \n",
        "    return final_val\n",
        "\n",
        "#For comparing results of networkx function and this one for eigenvector_centrality\n",
        "#It can be concluded that both functions return list where node with index 48 in df_1['id'] list is the most central.\n",
        "\n",
        "#G2 = nx.DiGraph(G)\n",
        "#eigen_centrality = nx.eigenvector_centrality(G2, max_iter=1000)\n",
        "#max_value = max(eigen_centrality, key=eigen_centrality.get)\n",
        "#print(ThisIsIndex_improved(df_1, max_value))\n",
        "\n",
        "#man_2 = eigenvector_centrality(G)\n",
        "#index_max_2 = 0\n",
        "#max_value_2 = 0\n",
        "#for i in range(len(man_2)):\n",
        "#  if(man_2[i] > max_value_2):\n",
        "#    max_value_2 = man_2[i]\n",
        "#    index_max_2 = i\n",
        "#print(index_max_2)\n",
        "#print(max_value_2)\n"
      ],
      "metadata": {
        "id": "wyJbxOKfxex8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}